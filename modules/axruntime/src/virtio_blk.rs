use core::cell::UnsafeCell;
use core::hint::spin_loop;
use core::ptr;
use core::sync::atomic::{fence, AtomicBool, AtomicUsize, Ordering};

use axfs::block::{BlockDevice, BlockId};
use axfs::{VfsError, VfsResult};

use crate::mm::{self, MemoryRegion};

const VIRTIO_MMIO_MAGIC: u32 = 0x7472_6976;
const VIRTIO_MMIO_VERSION: u32 = 2;
const VIRTIO_DEVICE_BLOCK: u32 = 2;
const VIRTIO_F_VERSION_1: u32 = 1 << 0;

const MMIO_MAGIC: usize = 0x000;
const MMIO_VERSION: usize = 0x004;
const MMIO_DEVICE_ID: usize = 0x008;
const MMIO_DEVICE_FEATURES: usize = 0x010;
const MMIO_DEVICE_FEATURES_SEL: usize = 0x014;
const MMIO_DRIVER_FEATURES: usize = 0x020;
const MMIO_DRIVER_FEATURES_SEL: usize = 0x024;
const MMIO_QUEUE_SEL: usize = 0x030;
const MMIO_QUEUE_NUM_MAX: usize = 0x034;
const MMIO_QUEUE_NUM: usize = 0x038;
const MMIO_QUEUE_READY: usize = 0x044;
const MMIO_QUEUE_NOTIFY: usize = 0x050;
const MMIO_STATUS: usize = 0x070;
const MMIO_QUEUE_DESC_LOW: usize = 0x080;
const MMIO_QUEUE_DESC_HIGH: usize = 0x084;
const MMIO_QUEUE_AVAIL_LOW: usize = 0x090;
const MMIO_QUEUE_AVAIL_HIGH: usize = 0x094;
const MMIO_QUEUE_USED_LOW: usize = 0x0a0;
const MMIO_QUEUE_USED_HIGH: usize = 0x0a4;

const STATUS_ACKNOWLEDGE: u32 = 1;
const STATUS_DRIVER: u32 = 2;
const STATUS_DRIVER_OK: u32 = 4;
const STATUS_FEATURES_OK: u32 = 8;
const STATUS_FAILED: u32 = 128;

const QUEUE_SIZE: usize = 8;
const SECTOR_SIZE: usize = 512;

const DESC_F_NEXT: u16 = 1;
const DESC_F_WRITE: u16 = 2;

const VIRTIO_BLK_T_IN: u32 = 0;
const VIRTIO_BLK_T_OUT: u32 = 1;

static VIRTIO_BLK_READY: AtomicBool = AtomicBool::new(false);
static VIRTIO_BLK_BASE: AtomicUsize = AtomicUsize::new(0);
static VIRTIO_BLK_QUEUE_SIZE: AtomicUsize = AtomicUsize::new(0);
static VIRTIO_BLK_USED_IDX: AtomicUsize = AtomicUsize::new(0);
static VIRTIO_QUEUE_LOCK: SpinLock = SpinLock::new();
static VIRTIO_BLK_DEVICE: VirtioBlkDevice = VirtioBlkDevice;
static VIRTIO_QUEUE: QueueCell = QueueCell::new();

pub struct VirtioBlkDevice;

impl BlockDevice for VirtioBlkDevice {
    fn block_size(&self) -> usize {
        SECTOR_SIZE
    }

    fn read_block(&self, block_id: BlockId, buf: &mut [u8]) -> VfsResult<()> {
        if buf.len() < SECTOR_SIZE {
            return Err(VfsError::Invalid);
        }
        submit_request(VIRTIO_BLK_T_IN, block_id, &mut buf[..SECTOR_SIZE])
    }

    fn write_block(&self, block_id: BlockId, buf: &[u8]) -> VfsResult<()> {
        if buf.len() < SECTOR_SIZE {
            return Err(VfsError::Invalid);
        }
        let mut scratch = [0u8; SECTOR_SIZE];
        scratch.copy_from_slice(&buf[..SECTOR_SIZE]);
        submit_request(VIRTIO_BLK_T_OUT, block_id, &mut scratch)
    }

    fn flush(&self) -> VfsResult<()> {
        Ok(())
    }
}

pub fn init(virtio_mmio: &[MemoryRegion]) {
    if VIRTIO_BLK_READY.load(Ordering::Acquire) {
        return;
    }
    for region in virtio_mmio {
        if region.size == 0 {
            continue;
        }
        if try_init_device(region.base as usize) {
            VIRTIO_BLK_READY.store(true, Ordering::Release);
            break;
        }
    }
}

pub fn device() -> Option<&'static VirtioBlkDevice> {
    if VIRTIO_BLK_READY.load(Ordering::Acquire) {
        Some(&VIRTIO_BLK_DEVICE)
    } else {
        None
    }
}

fn try_init_device(base: usize) -> bool {
    if mmio_read32(base, MMIO_MAGIC) != VIRTIO_MMIO_MAGIC {
        return false;
    }
    if mmio_read32(base, MMIO_VERSION) != VIRTIO_MMIO_VERSION {
        return false;
    }
    if mmio_read32(base, MMIO_DEVICE_ID) != VIRTIO_DEVICE_BLOCK {
        return false;
    }

    mmio_write32(base, MMIO_STATUS, 0);
    mmio_write32(base, MMIO_STATUS, STATUS_ACKNOWLEDGE);
    mmio_write32(base, MMIO_STATUS, STATUS_ACKNOWLEDGE | STATUS_DRIVER);

    let device_features = read_device_features(base);
    let mut driver_features: u64 = 0;
    if (device_features >> 32) & (VIRTIO_F_VERSION_1 as u64) != 0 {
        driver_features |= (VIRTIO_F_VERSION_1 as u64) << 32;
    }
    write_driver_features(base, driver_features);

    let status = mmio_read32(base, MMIO_STATUS) | STATUS_FEATURES_OK;
    mmio_write32(base, MMIO_STATUS, status);
    if (mmio_read32(base, MMIO_STATUS) & STATUS_FEATURES_OK) == 0 {
        mmio_write32(base, MMIO_STATUS, status | STATUS_FAILED);
        return false;
    }

    mmio_write32(base, MMIO_QUEUE_SEL, 0);
    let queue_max = mmio_read32(base, MMIO_QUEUE_NUM_MAX) as usize;
    if queue_max < 3 {
        return false;
    }
    let queue_size = core::cmp::min(queue_max, QUEUE_SIZE);
    mmio_write32(base, MMIO_QUEUE_NUM, queue_size as u32);

    let queue = VIRTIO_QUEUE.get();
    unsafe {
        ptr::write_bytes(queue as *mut VirtioBlkQueue, 0, 1);
    }
    let desc_addr = mm::kernel_virt_to_phys(queue.desc_addr()) as u64;
    let avail_addr = mm::kernel_virt_to_phys(queue.avail_addr()) as u64;
    let used_addr = mm::kernel_virt_to_phys(queue.used_addr()) as u64;

    mmio_write32(base, MMIO_QUEUE_DESC_LOW, desc_addr as u32);
    mmio_write32(base, MMIO_QUEUE_DESC_HIGH, (desc_addr >> 32) as u32);
    mmio_write32(base, MMIO_QUEUE_AVAIL_LOW, avail_addr as u32);
    mmio_write32(base, MMIO_QUEUE_AVAIL_HIGH, (avail_addr >> 32) as u32);
    mmio_write32(base, MMIO_QUEUE_USED_LOW, used_addr as u32);
    mmio_write32(base, MMIO_QUEUE_USED_HIGH, (used_addr >> 32) as u32);
    mmio_write32(base, MMIO_QUEUE_READY, 1);

    let status = mmio_read32(base, MMIO_STATUS) | STATUS_DRIVER_OK;
    mmio_write32(base, MMIO_STATUS, status);

    VIRTIO_BLK_BASE.store(base, Ordering::Release);
    VIRTIO_BLK_QUEUE_SIZE.store(queue_size, Ordering::Release);
    VIRTIO_BLK_USED_IDX.store(0, Ordering::Release);
    true
}

fn submit_request(req_type: u32, block_id: BlockId, buf: &mut [u8]) -> VfsResult<()> {
    if !VIRTIO_BLK_READY.load(Ordering::Acquire) {
        return Err(VfsError::NotSupported);
    }
    let _guard = VIRTIO_QUEUE_LOCK.lock();
    let base = VIRTIO_BLK_BASE.load(Ordering::Acquire);
    let queue_size = VIRTIO_BLK_QUEUE_SIZE.load(Ordering::Acquire);
    if base == 0 || queue_size == 0 {
        return Err(VfsError::NotSupported);
    }
    let queue = VIRTIO_QUEUE.get();
    let req = VirtioBlkReq {
        type_: req_type,
        reserved: 0,
        sector: block_id,
    };
    queue.req = req;
    queue.status = 0xff;

    let req_addr = mm::kernel_virt_to_phys(queue.req_addr()) as u64;
    let buf_addr = mm::kernel_virt_to_phys(buf.as_ptr() as usize) as u64;
    let status_addr = mm::kernel_virt_to_phys(queue.status_addr()) as u64;

    queue.desc[0] = VirtqDesc {
        addr: req_addr,
        len: core::mem::size_of::<VirtioBlkReq>() as u32,
        flags: DESC_F_NEXT,
        next: 1,
    };
    queue.desc[1] = VirtqDesc {
        addr: buf_addr,
        len: buf.len() as u32,
        flags: if req_type == VIRTIO_BLK_T_IN {
            DESC_F_WRITE | DESC_F_NEXT
        } else {
            DESC_F_NEXT
        },
        next: 2,
    };
    queue.desc[2] = VirtqDesc {
        addr: status_addr,
        len: 1,
        flags: DESC_F_WRITE,
        next: 0,
    };

    let avail_idx = queue.avail.idx;
    queue.avail.ring[(avail_idx as usize) % queue_size] = 0;
    fence(Ordering::SeqCst);
    queue.avail.idx = avail_idx.wrapping_add(1);

    fence(Ordering::SeqCst);
    mmio_write32(base, MMIO_QUEUE_NOTIFY, 0);

    let mut last_used = VIRTIO_BLK_USED_IDX.load(Ordering::Acquire) as u16;
    loop {
        let used_idx = unsafe { ptr::read_volatile(&queue.used.idx) };
        if used_idx != last_used {
            last_used = used_idx;
            VIRTIO_BLK_USED_IDX.store(last_used as usize, Ordering::Release);
            break;
        }
        spin_loop();
    }

    let status = unsafe { ptr::read_volatile(&queue.status) };
    if status != 0 {
        return Err(VfsError::Io);
    }
    Ok(())
}

#[repr(C, align(16))]
#[derive(Clone, Copy)]
struct VirtqDesc {
    addr: u64,
    len: u32,
    flags: u16,
    next: u16,
}

#[repr(C, align(2))]
struct VirtqAvail {
    flags: u16,
    idx: u16,
    ring: [u16; QUEUE_SIZE],
    used_event: u16,
}

#[repr(C, align(4))]
#[derive(Clone, Copy)]
struct VirtqUsedElem {
    id: u32,
    len: u32,
}

#[repr(C, align(4))]
struct VirtqUsed {
    flags: u16,
    idx: u16,
    ring: [VirtqUsedElem; QUEUE_SIZE],
    avail_event: u16,
}

#[repr(C)]
#[derive(Clone, Copy)]
struct VirtioBlkReq {
    type_: u32,
    reserved: u32,
    sector: u64,
}

#[repr(C, align(4096))]
struct VirtioBlkQueue {
    desc: [VirtqDesc; QUEUE_SIZE],
    avail: VirtqAvail,
    used: VirtqUsed,
    req: VirtioBlkReq,
    status: u8,
}

struct QueueCell {
    inner: UnsafeCell<VirtioBlkQueue>,
}

impl QueueCell {
    const fn new() -> Self {
        const ZERO_DESC: VirtqDesc = VirtqDesc {
            addr: 0,
            len: 0,
            flags: 0,
            next: 0,
        };
        const ZERO_USED: VirtqUsedElem = VirtqUsedElem { id: 0, len: 0 };
        Self {
            inner: UnsafeCell::new(VirtioBlkQueue {
                desc: [ZERO_DESC; QUEUE_SIZE],
                avail: VirtqAvail {
                    flags: 0,
                    idx: 0,
                    ring: [0; QUEUE_SIZE],
                    used_event: 0,
                },
                used: VirtqUsed {
                    flags: 0,
                    idx: 0,
                    ring: [ZERO_USED; QUEUE_SIZE],
                    avail_event: 0,
                },
                req: VirtioBlkReq {
                    type_: 0,
                    reserved: 0,
                    sector: 0,
                },
                status: 0,
            }),
        }
    }

    fn get(&self) -> &mut VirtioBlkQueue {
        // SAFETY: 仅通过全局自旋锁串行访问。
        unsafe { &mut *self.inner.get() }
    }
}

unsafe impl Sync for QueueCell {}

struct SpinLock {
    locked: AtomicBool,
}

impl SpinLock {
    const fn new() -> Self {
        Self {
            locked: AtomicBool::new(false),
        }
    }

    fn lock(&self) -> SpinGuard<'_> {
        while self
            .locked
            .compare_exchange(false, true, Ordering::Acquire, Ordering::Relaxed)
            .is_err()
        {
            spin_loop();
        }
        SpinGuard { lock: self }
    }
}

struct SpinGuard<'a> {
    lock: &'a SpinLock,
}

impl Drop for SpinGuard<'_> {
    fn drop(&mut self) {
        self.lock.locked.store(false, Ordering::Release);
    }
}

fn mmio_read32(base: usize, offset: usize) -> u32 {
    unsafe { ptr::read_volatile((base + offset) as *const u32) }
}

fn mmio_write32(base: usize, offset: usize, value: u32) {
    unsafe { ptr::write_volatile((base + offset) as *mut u32, value) }
}

fn read_device_features(base: usize) -> u64 {
    mmio_write32(base, MMIO_DEVICE_FEATURES_SEL, 0);
    let low = mmio_read32(base, MMIO_DEVICE_FEATURES) as u64;
    mmio_write32(base, MMIO_DEVICE_FEATURES_SEL, 1);
    let high = mmio_read32(base, MMIO_DEVICE_FEATURES) as u64;
    (high << 32) | low
}

fn write_driver_features(base: usize, features: u64) {
    mmio_write32(base, MMIO_DRIVER_FEATURES_SEL, 0);
    mmio_write32(base, MMIO_DRIVER_FEATURES, features as u32);
    mmio_write32(base, MMIO_DRIVER_FEATURES_SEL, 1);
    mmio_write32(base, MMIO_DRIVER_FEATURES, (features >> 32) as u32);
}

impl VirtioBlkQueue {
    fn desc_addr(&self) -> usize {
        self.desc.as_ptr() as usize
    }

    fn avail_addr(&self) -> usize {
        &self.avail as *const _ as usize
    }

    fn used_addr(&self) -> usize {
        &self.used as *const _ as usize
    }

    fn req_addr(&self) -> usize {
        &self.req as *const _ as usize
    }

    fn status_addr(&self) -> usize {
        &self.status as *const _ as usize
    }
}
